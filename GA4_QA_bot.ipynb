{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yuukimotai/GA4_QABot/blob/main/GA4_QA_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdTAKHV1N2wy",
    "outputId": "3cf0a701-668c-4787-b717-74ee373a31a0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNwc5coCT-ft"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "origin = os.listdir('kftt-data-1.0/data/orig')\n",
    "token = os.listdir('kftt-data-1.0/data/tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmvWu61HxkMk"
   },
   "outputs": [],
   "source": [
    "originList = []# origフォルダに入っていたものから抽出する\n",
    "for ori in origin:\n",
    "  if ori.endswith('.ja'):\n",
    "    originList.append(ori)\n",
    "target_file = open('kftt_origin_ja.txt', 'w')\n",
    "for i in range(len(originList)):\n",
    "  f = open(\"kftt-data-1.0/data/orig/\" + originList[i], \"rb\")\n",
    "\n",
    "  data = f.read()\n",
    "\n",
    "  utf8_data = data.decode(encoding='utf-8')\n",
    "  print(utf8_data)\n",
    "  target_file.write(utf8_data)\n",
    "  f.close()\n",
    "target_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhY_I_yJAC2q"
   },
   "outputs": [],
   "source": [
    "with open('rakuten_book_000.txt', 'r') as f:\n",
    "  origin_ja = f.readlines()\n",
    "origin_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mecab\n",
    "#export MECABRC=\"[インストール先]/etc/mecabrc\"\n",
    "#/usr/lib/aarch64-linux-gnu/mecab/dic/ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "# /usr/lib/aarch64-linux-gnu/mecab/dic/ipadic\n",
    "tokenizer = MeCab.Tagger('-Owakati -d \"/usr/lib/aarch64-linux-gnu/mecab/dic/ipadic\" -u \"/usr/lib/aarch64-linux-gnu/mecab/dic/ipadic/NEologd.20200910-u.dic\"')\n",
    "print(tokenizer.parse('特急はくたか'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rakuten_books_tokenized.txt\", mode=\"a\") as f:\n",
    "    for sentence in origin_ja:\n",
    "        f.write(tokenizer.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rakuten_book_000.txt\", mode=\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 5000行ごとに分割\n",
    "chunk_size = 1000\n",
    "chunks = [lines[i:i + chunk_size] for i in range(0, len(lines), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    with open(f\"rakutenbooks_chunk_{i+1}.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(chunk)\n",
    "\n",
    "print(f\"分割完了: {len(chunks)}個のファイルが作成されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rakuten_books_tokenized.txt\", mode=\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 5000行ごとに分割\n",
    "chunk_size = 5000\n",
    "chunks = [lines[i:i + chunk_size] for i in range(0, len(lines), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    with open(f\"rakutenbookstoken_chunk_{i+1}.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(chunk)\n",
    "\n",
    "print(f\"分割完了: {len(chunks)}個のファイルが作成されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"rakutenbooks_chunk_4.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    chunk = f.read()\n",
    "\n",
    "train, temp = train_test_split(chunk, test_size=0.2)\n",
    "validation, test = train_test_split(temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'function'=<function tokenize at 0xffff247ed550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100% 1000/1000 [00:00<00:00, 3596.45 examples/s]\n",
      "Map: 100% 1000/1000 [00:00<00:00, 4106.80 examples/s]\n",
      "Map: 100% 1000/1000 [00:00<00:00, 4844.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 5:40:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.749500</td>\n",
       "      <td>0.622454</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.741682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.628679</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.738637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.790660</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.772901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.967295</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.755455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.763516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.35394152866601947, metrics={'train_runtime': 20431.3564, 'train_samples_per_second': 0.245, 'train_steps_per_second': 0.031, 'total_flos': 1315567088640000.0, 'train_loss': 0.35394152866601947, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(30)\n",
    "dataset = DatasetDict({\n",
    "    \"train\"     : dataset['train']\\\n",
    "        .select(random.sample(range(dataset['train']     .num_rows), k=1000)),\n",
    "    \"validation\": dataset['validation']\\\n",
    "        .select(random.sample(range(dataset['validation'].num_rows), k=1000)),\n",
    "    \"test\"      : dataset['test']\\\n",
    "        .select(random.sample(range(dataset['test']      .num_rows), k=1000))\n",
    "})\n",
    "# トークナイザのロード\n",
    "model_ckpt =\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "# 事前学習モデルのロード\n",
    "model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "access_token = \"\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 3\n",
    "model = (AutoModelForSequenceClassification\n",
    "    .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "    .to(device))\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = \"sample-text-classification-bert\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    log_level=\"error\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"cl-tohoku_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "access_token = \"\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2, token=access_token)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "with open(f\"rakutenbookstoken_chunk_{1}.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    tokens =  f.readlines()\n",
    "for i in range(len(chunks)):\n",
    "    with open(f\"rakutenbooks_chunk_{i+1}.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        lines =  f.readlines()\n",
    "    inputs = tokenizer(lines, return_tensors='pt', padding=True, truncation=True)\n",
    "    print(len(inputs[\"input_ids\"]))\n",
    "    # トレーニングデータセットの作成\n",
    "    dataset = Dataset.from_dict({'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': lines})\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f\"rakutenbooks_model_chunk_{i+1}\")\n",
    "\n",
    "print(\"ファインチューニング完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "access_token = \"\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2, token=access_token)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "with open(f\"rakutenbookstoken_chunk_{1}.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    tokens =  f.readlines()\n",
    "for i in range(len(chunks)):\n",
    "    with open(f\"rakutenbooks_chunk_{i+1}.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        lines =  f.readlines()\n",
    "    inputs = tokenizer(lines, return_tensors='pt', padding=True, truncation=True)\n",
    "    print(len(inputs[\"input_ids\"]))\n",
    "    # トレーニングデータセットの作成\n",
    "    dataset = Dataset.from_dict({'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': lines})\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(f\"rakutenbooks_model_chunk_{i+1}\")\n",
    "\n",
    "print(\"ファインチューニング完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26Yv8sz-DJbH",
    "outputId": "6a1f7758-cff3-4ea6-f3b3-6acef78eaf45"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞ\\\n",
    "ただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽ\\\n",
    "まみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "katakana = \"ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾ\\\n",
    "タダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポ\\\n",
    "マミムメモャヤュユョヨラリルレロヮワヰヱヲンヴ\"\n",
    "\n",
    "chars = hiragana + katakana\n",
    "\n",
    "for char in origin_ja:  # ひらがな、カタカナ以外でコーパスに使われている文字を追加\n",
    "    if char not in chars:\n",
    "        chars += char\n",
    "\n",
    "chars += \"\\t\\n\"  # タブと改行を追加\n",
    "\n",
    "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
    "print(chars_list)\n",
    "\n",
    "with open(\"chumonno_oi_ryoriten.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
    "    pickle.dump(chars_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzQzaNaeM9ZD",
    "outputId": "f42babcd-0f3d-4774-e47b-fcb2f83e33bd"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('chumonno_oi_ryoriten.pickle', mode='rb') as f:\n",
    "    chars_list = pickle.load(f)\n",
    "print(chars_list)\n",
    "\n",
    "def is_invalid(message):\n",
    "    is_invalid =False\n",
    "    for char in message:\n",
    "        if char not in chars_list:\n",
    "            is_invalid = True\n",
    "    return is_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wNtQeFmNRP-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "\n",
    "n_char = len(chars_list)\n",
    "max_length_x = 128\n",
    "\n",
    "# 文章をone-hot表現に変換する関数\n",
    "# def sentence_to_vector(sentence):\n",
    "#     vector = np.zeros((1, max_length_x, n_char), dtype=np.bool_)\n",
    "#     for j, char in enumerate(sentence):\n",
    "#         vector[0][j][char_indices[char]] = 1\n",
    "#     return vector\n",
    "def sentence_to_vector(sentence):\n",
    "    vector = np.zeros((1, max_length_x, n_char), dtype=np.bool_)\n",
    "    for j, char in enumerate(sentence):\n",
    "        if char in char_indices:\n",
    "            vector[0][j][char_indices[char]] = 1\n",
    "        else:\n",
    "            print(f\"Character '{char}' not in char_indices.\")\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hi2qzOMERKhL",
    "outputId": "30a3d35c-12e8-4f9a-9719-d662b49b4fae"
   },
   "outputs": [],
   "source": [
    "#!wget -P /content/drive/MyDrive/nlp_dataset/ http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZZwf9NvS-FV"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# tar.gzファイルを開く\n",
    "with tarfile.open('drive/MyDrive/nlp_dataset/kftt-data-1.0.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path=\"drive/MyDrive/nlp_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pBZ-Xnr2I-F",
    "outputId": "ea61e5b7-5da5-4ca7-8065-78091ea5bfc8"
   },
   "outputs": [],
   "source": [
    "tokenList = []# tokフォルダに入っていたものから抽出する\n",
    "for tok in token:\n",
    "  if tok.endswith('.ja'):\n",
    "    tokenList.append(tok)\n",
    "target_file = open('kftt_token_ja.txt', 'w')\n",
    "for i in range(len(tokenList)):\n",
    "  f = open(\"drive/MyDrive/nlp_dataset/kftt-data-1.0/data/tok/\" + tokenList[i], \"rb\")\n",
    "\n",
    "  data = f.read()\n",
    "\n",
    "  utf8_data = data.decode(encoding='utf-8')\n",
    "  print(utf8_data)\n",
    "  target_file.write(utf8_data)\n",
    "  f.close()\n",
    "target_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8KlKS12C6Nv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "origin_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6JymSJYEuL3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}  # 文字がキーでインデックスが値\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}  # インデックスがキーで文字が値\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "\n",
    "seperator = \" \"\n",
    "sentence_list = origin_ja.split(seperator)\n",
    "print(sentence_list)\n",
    "sentence_list.pop()\n",
    "sentence_list = [x+seperator for x in sentence_list]\n",
    "\n",
    "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
    "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
    "\n",
    "n_char = len(chars_list)  # 文字の種類の数\n",
    "n_sample = len(sentence_list) - 1  # サンプル数\n",
    "\n",
    "x_sentences = []  # 入力の文章\n",
    "t_sentences = []  # 正解の文章\n",
    "for i in range(n_sample):\n",
    "    x_sentences.append(sentence_list[i])\n",
    "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
    "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
    "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
    "\n",
    "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool_)  # encoderへの入力\n",
    "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool_)  # decoderへの入力\n",
    "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool_)  # decoderの正解\n",
    "\n",
    "for i in range(n_sample):\n",
    "    x_sentence = x_sentences[i]\n",
    "    t_sentence = t_sentences[i]\n",
    "    for j, char in enumerate(x_sentence):\n",
    "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
    "    for j, char in enumerate(t_sentence):\n",
    "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
    "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
    "            t_decoder[i, j-1, char_indices[char]] = 1\n",
    "\n",
    "print(x_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dix0OkwhE9ZF"
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "epochs = 200\n",
    "n_mid = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aoUQziZFEVh",
    "outputId": "268cb670-3cc2-407e-98de-860e0a10b109"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GRU, Input, Masking\n",
    "\n",
    "encoder_input = Input(shape=(None, n_char))\n",
    "encoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "encoder_masked = encoder_mask(encoder_input)\n",
    "encoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_state=True)  # dropoutを設定し、ニューロンをランダムに無効にする\n",
    "encoder_output, encoder_state_h = encoder_lstm(encoder_masked)\n",
    "\n",
    "decoder_input = Input(shape=(None, n_char))\n",
    "decoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "decoder_masked = decoder_mask(decoder_input)\n",
    "decoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)  # dropoutを設定\n",
    "decoder_output, _ = decoder_lstm(decoder_masked, initial_state=encoder_state_h)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_char, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8znP1eutF5fg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.1,  # 10%は検証用\n",
    "                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "sn6FDN_LGRET",
    "outputId": "99bd99f3-05c4-4cf6-bf99-129d93043dbc"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.plot(np.arange(len(val_loss)), val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfmMAvaFPnDS"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "# encoderのモデル\n",
    "encoder_model = Model(encoder_input, encoder_state_h)\n",
    "\n",
    "# decoderのモデル\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h]\n",
    "\n",
    "decoder_output, decoder_state_h = decoder_lstm(decoder_input,\n",
    "                                               initial_state=decoder_state_in_h)\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_state_in,\n",
    "                      [decoder_output, decoder_state_h])\n",
    "\n",
    "# モデルの保存\n",
    "encoder_model.save('encoder_chumonno_oi_ryouriten_transed.keras')\n",
    "decoder_model.save('decoder_chumonno_oi_ryouriten_transed.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5xTJDVeQIc2",
    "outputId": "86d7f13b-5eef-4eb3-fa79-0860f5994a8c"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "encoder_model = load_model('encoder_chumonno_oi_ryouriten_transed.keras')\n",
    "decoder_model = load_model('decoder_chumonno_oi_ryouriten_transed.keras')\n",
    "\n",
    "def respond(message, beta=5):\n",
    "    vec = sentence_to_vector(message)  # 文字列をone-hot表現に変換\n",
    "    print(vec)\n",
    "    state_value = encoder_model.predict(vec)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices['\\t']] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power))\n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "\n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "\n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "        print(\"以下はrespond_sentence\")\n",
    "        print(respond_sentence)\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "UwC8h7GoLjJ3",
    "outputId": "d44fe0d8-74a5-462a-bc54-b40e10c6ad83"
   },
   "outputs": [],
   "source": [
    "bot_name = \"賢治bot\"\n",
    "your_name = input(\"おなまえをおしえてください。:\")\n",
    "print()\n",
    "\n",
    "print(bot_name + \": \" + \"こんにちは、\" + your_name + \"さん。\")\n",
    "message = \"\"\n",
    "while message != \"さようなら。\":\n",
    "\n",
    "    while True:\n",
    "        message = input(your_name + \": \")\n",
    "        if not is_invalid(message):\n",
    "            break\n",
    "        else:\n",
    "            print(bot_name + \": ひらがなか、カタカナをつかってください。\")\n",
    "\n",
    "    response = respond(message)\n",
    "    print(bot_name + \": \" + response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhpfjKHC12E7CapWnv42ri",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
